{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1806b4d-613a-4520-aa3e-39b4d5100233",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\r\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/62.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:19\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/62.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:04\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/62.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/62.5 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/62.5 MB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/62.5 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/62.5 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.2/62.5 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/62.5 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/62.5 MB\u001b[0m \u001b[31m155.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/62.5 MB\u001b[0m \u001b[31m144.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/62.5 MB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m47.1/62.5 MB\u001b[0m \u001b[31m147.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m52.0/62.5 MB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m57.0/62.5 MB\u001b[0m \u001b[31m135.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.0/62.5 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m140.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.5 in /databricks/python3/lib/python3.11/site-packages (from opencv-python) (1.23.5)\r\n",
      "Installing collected packages: opencv-python\r\n",
      "Successfully installed opencv-python-4.10.0.84\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Collecting ultralytics\r\n",
      "  Downloading ultralytics-8.2.35-py3-none-any.whl (782 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/782.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/782.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m593.9/782.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.8/782.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (2.31.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (6.0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (4.65.0)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (0.12.2)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (0.17.2+cpu)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (3.7.2)\r\n",
      "Collecting ultralytics-thop>=2.0.0\r\n",
      "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (1.11.1)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (2.2.2+cpu)\r\n",
      "Requirement already satisfied: py-cpuinfo in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (8.0.0)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-bb9e8fd0-e5a3-4339-aab5-eadab9f75ce4/lib/python3.11/site-packages (from ultralytics) (4.10.0.84)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (1.5.3)\r\n",
      "Requirement already satisfied: numpy<2.0.0 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (1.23.5)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (9.4.0)\r\n",
      "Requirement already satisfied: psutil in /databricks/python3/lib/python3.11/site-packages (from ultralytics) (5.9.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.0.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2022.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (1.26.16)\r\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.10.0)\r\n",
      "Requirement already satisfied: fsspec in /databricks/python3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2023.5.0)\r\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /databricks/python3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /databricks/python3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Installing collected packages: ultralytics-thop, ultralytics\r\n",
      "Successfully installed ultralytics-8.2.35 ultralytics-thop-2.0.0\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Requirement already satisfied: boto3 in /databricks/python3/lib/python3.11/site-packages (1.34.39)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.11/site-packages (from boto3) (0.10.0)\r\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.39 in /databricks/python3/lib/python3.11/site-packages (from boto3) (1.34.39)\r\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.11/site-packages (from boto3) (0.10.1)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.39->boto3) (2.8.2)\r\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /databricks/python3/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.39->boto3) (1.26.16)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.39->boto3) (1.16.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python\n",
    "!pip3 install ultralytics\n",
    "!pip3 install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8232389-7403-4176-822f-c14c50b8b7ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id='XXXX',\n",
    "    aws_secret_access_key='XXXX'\n",
    ")\n",
    "\n",
    "# Print out bucket names\n",
    "# for bucket in s3_client.buckets.all():\n",
    "#     print(bucket.name)\n",
    "\n",
    "def upload_file( bucket_name , file_path , object_name):\n",
    "    print(\"upload file function\")\n",
    "    try:\n",
    "        s3_client.upload_file(file_path , bucket_name ,object_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def download_file( bucket_name , file_path , object_name):\n",
    "    try:\n",
    "        s3_client.download_file( bucket_name ,object_name,file_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def delete_file(bucket_name , file_name):\n",
    "    try:\n",
    "        s3_client.delete_object(Bucket= bucket_name , Key=file_name)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f092e42a-56fa-4232-aa11-19684c890a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_file( 'demo-test-pipeline' , 'ss_2.mp4' , 'ss_2.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2987211-e98d-4dbf-ab1d-57dad854fda9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filler stuff\n"
     ]
    }
   ],
   "source": [
    "print(\"filler stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86d7d221-56a0-452b-81bd-03be44c87f37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2 \n",
    "import sys\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "\n",
    "def detector_model(image):\n",
    "    results = model.predict([image], verbose=False , conf = 0.3)  \n",
    "\n",
    "    cls_names = model.names\n",
    "    person = 0\n",
    "    vehicle = 0\n",
    "    # Iterate over the predictions\n",
    "    for i, result in enumerate(results):\n",
    "        # Extract boxes, scores and class labels\n",
    "        bboxes = result.boxes.xyxy.cpu().numpy()\n",
    "        scores = result.boxes.conf.cpu().numpy()\n",
    "        cls = result.boxes.cls.cpu().numpy()\n",
    "        \n",
    "        # Iterate over the bboxes, filter by confidence and draw over the image\n",
    "        for j in range(len(bboxes)):\n",
    "            (x1, y1, x2, y2), score, c = bboxes[j], scores[j], cls[j]\n",
    "            if int(c)==0:\n",
    "                person = person + 1\n",
    "            elif int(c) in [1,2,3,4,5,6,7,8]:\n",
    "                vehicle = vehicle + 1\n",
    "            if score >= 0.3:\n",
    "                cv2.putText(image, f\"{cls_names[c]} {score:.4f}\",(int(x1), int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (int(c == 0) * 255, int(c == 1) * 255, int(c ==2) * 255), 2)\n",
    "\n",
    "        # Save image with detections\n",
    "    return image , person , vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1156c7d1-a84b-4cbe-9750-f22e6c936e6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+-------------+\n",
      "|frame_id|          frame_data|person_count|vehicle_count|\n",
      "+--------+--------------------+------------+-------------+\n",
      "|       0|[FF D8 FF E0 00 1...|           1|           18|\n",
      "|       1|[FF D8 FF E0 00 1...|           2|           17|\n",
      "|       2|[FF D8 FF E0 00 1...|           2|           16|\n",
      "|       3|[FF D8 FF E0 00 1...|           3|           17|\n",
      "|       4|[FF D8 FF E0 00 1...|           3|           21|\n",
      "|       5|[FF D8 FF E0 00 1...|           3|           20|\n",
      "|       6|[FF D8 FF E0 00 1...|           2|           17|\n",
      "|       7|[FF D8 FF E0 00 1...|           2|           19|\n",
      "|       8|[FF D8 FF E0 00 1...|           3|           20|\n",
      "|       9|[FF D8 FF E0 00 1...|           2|           16|\n",
      "|      10|[FF D8 FF E0 00 1...|           1|           16|\n",
      "|      11|[FF D8 FF E0 00 1...|           2|           19|\n",
      "|      12|[FF D8 FF E0 00 1...|           1|           17|\n",
      "|      13|[FF D8 FF E0 00 1...|           1|           16|\n",
      "|      14|[FF D8 FF E0 00 1...|           2|           17|\n",
      "|      15|[FF D8 FF E0 00 1...|           2|           13|\n",
      "|      16|[FF D8 FF E0 00 1...|           1|           14|\n",
      "|      17|[FF D8 FF E0 00 1...|           2|           15|\n",
      "|      18|[FF D8 FF E0 00 1...|           2|           19|\n",
      "|      19|[FF D8 FF E0 00 1...|           2|           20|\n",
      "+--------+--------------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, BinaryType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"VideoToDataFrame\").getOrCreate()\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"frame_id\", IntegerType(), True),\n",
    "    StructField(\"frame_data\", BinaryType(), True),\n",
    "    StructField(\"person_count\", IntegerType(), True),\n",
    "    StructField(\"vehicle_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define function to read frames from video\n",
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_id = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame , person , vehicle = detector_model(frame)\n",
    "        ret, buffer = cv2.imencode('.jpg', frame)\n",
    "        frames.append((frame_id, buffer.tobytes(),person , vehicle))\n",
    "        frame_id += 1\n",
    "        if frame_id ==300:\n",
    "            break\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Create an RDD from the video frames\n",
    "video_path = \"ss_2.mp4\"\n",
    "#\"/Volumes/trial_workspace/default/test/ss_2.mp4\"\n",
    "frames_rdd = spark.sparkContext.parallelize(read_video_frames(video_path))\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "df = frames_rdd.toDF(schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52482128-a182-4a39-901c-6a12b70ec584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mdemo\u001b[0m*  \u001b[01;32merr.txt\u001b[0m*  \u001b[01;32mmeta_info.csv\u001b[0m*  \u001b[01;32moutput_video.mp4\u001b[0m*  \u001b[01;32mss_2.mp4\u001b[0m*  \u001b[01;32myolov8n.pt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f78351-8809-4c93-8a0c-98a94499b5c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.collect()\n",
    "#df1[0].__getitem__('frame_data') \n",
    "nparr = np.frombuffer(df1[0].__getitem__('frame_data'), np.uint8)\n",
    "frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "height, width  = 480 , 640  #, _ = frame.shape\n",
    "video_writer = cv2.VideoWriter(\"output_video.mp4\", \n",
    "                                cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                                20, \n",
    "                                (width,height))\n",
    "meta_info = []\n",
    "for i in range(len(df1)):\n",
    "    nparr = np.frombuffer(df1[i].__getitem__('frame_data'), np.uint8)\n",
    "    frame_id , p_count , v_count = [df1[i].__getitem__(j) for j in ['frame_id', 'person_count','vehicle_count']]\n",
    "    meta_info.append([frame_id , p_count , v_count])\n",
    "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "    #print(frame.shape)\n",
    "    frame= cv2.resize(frame,(width,height))\n",
    "    video_writer.write(frame)\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25648dcc-0bbd-4c42-a9ee-aa816e57bc1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload file function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_file( 'demo-test-pipeline' , 'output_video.mp4' , 'output_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd81419-7594-4e9e-8172-e3e78ffa7843",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!rm -r output_video.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b7a92f-62ec-4b85-8b98-444d9ef87d3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df_pd = pd.DataFrame(meta_info , columns = ['frame_id', 'person_count','vehicle_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004bfe46-3ba0-4aaa-b306-c703e9442428",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd.to_csv('meta_info.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94d067c-30fd-49f9-ac0b-46b401172179",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mdemo\u001b[0m*  \u001b[01;32merr.txt\u001b[0m*  \u001b[01;32mmeta_info.csv\u001b[0m*  \u001b[01;32moutput_video.mp4\u001b[0m*  \u001b[01;32mss_2.mp4\u001b[0m*  \u001b[01;32myolov8n.pt\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550fc00a-f3e0-4b01-88be-409a6951585a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload file function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_file( 'demo-test-pipeline' , 'meta_info.csv', 'meta_info.csv')\n",
    "#upload_file( 'demo-test-pipeline' , 'meta_info.csv', 'output_folder/meta_info.csv' ) #bucket_name , file_path , object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc74d396-37cf-4805-a84b-9da6d14bb700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting snowflake-connector-python\n",
      "  Downloading snowflake_connector_python-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/2.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/2.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/2.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m2.2/2.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/105.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (2022.7)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (4.10.0)\n",
      "Requirement already satisfied: cryptography<43.0.0,>=3.1.0 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.3)\n",
      "Collecting sortedcontainers>=2.4.0\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (1.15.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (2.0.4)\n",
      "Collecting tomlkit\n",
      "  Downloading tomlkit-0.12.5-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (3.9.0)\n",
      "Requirement already satisfied: pyOpenSSL<25.0.0,>=16.2.0 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: requests<3.0.0 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0->snowflake-connector-python) (1.26.16)\n",
      "Installing collected packages: sortedcontainers, asn1crypto, tomlkit, snowflake-connector-python\n",
      "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.10.1 sortedcontainers-2.4.0 tomlkit-0.12.5\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27ad022-ffbb-4bcd-bb52-064bb5b97eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FRAME_ID  PERSON_COUNT  VEHICLE_COUNT\n",
      "0         0             1             18\n",
      "1         1             2             17\n",
      "2         2             2             16\n",
      "3         3             3             17\n",
      "4         4             3             21\n",
      "5         5             3             20\n",
      "6         6             2             17\n",
      "7         7             2             19\n",
      "8         8             3             20\n",
      "9         9             2             16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 1,\n",
       " 300,\n",
       " [('wyqexiykrm/file0.txt', 'LOADED', 300, 300, 1, 0, None, None, None, None)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snowflake.connector as snow\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "conn = snow.connect(\n",
    "    user= 'prathik30',\n",
    "    password= 'XXXXX',\n",
    "    account= 'XXXXX',\n",
    "    warehouse= 'compute_wh',\n",
    "    database= 'DW_COURSE_DB',\n",
    "    schema= 'VIDEO_STREAM',\n",
    ")\n",
    "cur = conn.cursor()\n",
    "df_pd.columns = ['FRAME_ID', 'PERSON_COUNT', 'VEHICLE_COUNT']\n",
    "print(df_pd.head(10))\n",
    "write_pandas(conn, df_pd, \"STREAMCOUNT\",overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c4448b-5e53-4d52-971d-6c10ded02742",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
